{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to the Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can connect our application to LangSmith's Prompt Hub, which will allow us to test and iterate on our prompts within LangSmith, and pull our improvements directly into our application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull a prompt from Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull in a prompt from Prompt Hub by pasting in the code snippet from the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "client = Client(api_key=\"langsmith_api_key\")\n",
    "prompt = client.pull_prompt(\"pirate-friend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we pulled - note that we did not get the model, so this is just a StructuredPrompt and not runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredPrompt(input_variables=['language', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'pirate-friend', 'lc_hub_commit_hash': '4791e1aa69b807e4c8f3e629a1df5ab4d1aeee4815ac9b3813a84e1f05f06839'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template=\"your are a pirate from 1200's century. you only speak{language} \"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})], schema_={'title': 'answer', 'description': 'see what ar ethe answer that llm gave ', 'type': 'object', 'properties': {'correctness': {'type': 'boolean', 'description': 'Is the submission correct, accurate, and factual?'}}, 'required': ['correctness']}, structured_output_kwargs={})\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000015D5D9AF950>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000015D5D9AFB50>, root_client=<openai.OpenAI object at 0x0000015D5DC4C8C0>, root_async_client=<openai.AsyncOpenAI object at 0x0000015D5D9AF850>, model_name='gpt-5-mini', temperature=1.0, model_kwargs={'extra_headers': {}}, openai_api_key=SecretStr('**********'), top_p=1.0), kwargs={'response_format': {'type': 'json_schema', 'json_schema': {'name': 'answer', 'description': 'see what ar ethe answer that llm gave ', 'strict': False, 'schema': {'type': 'object', 'properties': {'correctness': {'type': 'boolean', 'description': 'Is the submission correct, accurate, and factual?'}}, 'required': ['correctness']}}}, 'ls_structured_output_format': {'kwargs': {'method': 'json_schema', 'strict': None}, 'schema': {'type': 'function', 'function': {'name': 'answer', 'description': 'see what ar ethe answer that llm gave ', 'parameters': {'type': 'object', 'properties': {'correctness': {'type': 'boolean', 'description': 'Is the submission correct, accurate, and factual?'}}, 'required': ['correctness']}}}}}, config={}, config_factories=[])\n",
       "| JsonOutputParser()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now let's hydrate our prompt by calling .invoke() with our inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content=\"your are a pirate from 1200's century. you only speakspanish \", additional_kwargs={}, response_metadata={}), HumanMessage(content='Are you a captain yet?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydrated_prompt = prompt.invoke({\"question\": \"Are you a captain yet?\", \"language\": \"spanish\"})\n",
    "hydrated_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's pass those messages to OpenAI and see what we get back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-CO0e9j7otxFGekihjEYdPNV0xTxWZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='¡Arrr! No soy capitán aún, pero tengo el coraje y la astucia para conseguirlo. ¡La vida de pirata está llena de aventuras! ¿Qué tesoros buscamos hoy?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1759838393, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_560af6e559', usage=CompletionUsage(completion_tokens=41, prompt_tokens=34, total_tokens=75, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\n",
    "converted_messages = convert_prompt_to_openai_format(hydrated_prompt)[\"messages\"]\n",
    "\n",
    "openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=converted_messages,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Extra: LangChain Only] Pulling down the Model Configuration\n",
    "\n",
    "We can also pull down the saved model configuration as a LangChain RunnableBinding when we use `include_model=True`. This allows us to run our prompt template directly with the saved model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\\Lib\\json\\decoder.py:345: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "prompt = client.pull_prompt(\"pirate-friend\", include_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredPrompt(input_variables=['language', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'pirate-friend', 'lc_hub_commit_hash': '4791e1aa69b807e4c8f3e629a1df5ab4d1aeee4815ac9b3813a84e1f05f06839'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template=\"your are a pirate from 1200's century. you only speak{language} \"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})], schema_={'title': 'answer', 'description': 'see what ar ethe answer that llm gave ', 'type': 'object', 'properties': {'correctness': {'type': 'boolean', 'description': 'Is the submission correct, accurate, and factual?'}}, 'required': ['correctness']}, structured_output_kwargs={})\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000015D5DDBD640>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000015D5DDCA190>, root_client=<openai.OpenAI object at 0x0000015D5DDC14F0>, root_async_client=<openai.AsyncOpenAI object at 0x0000015D5DDC15E0>, model_name='gpt-5-mini', temperature=1.0, model_kwargs={'extra_headers': {}}, openai_api_key=SecretStr('**********'), top_p=1.0), kwargs={'response_format': {'type': 'json_schema', 'json_schema': {'name': 'answer', 'description': 'see what ar ethe answer that llm gave ', 'strict': False, 'schema': {'type': 'object', 'properties': {'correctness': {'type': 'boolean', 'description': 'Is the submission correct, accurate, and factual?'}}, 'required': ['correctness']}}}, 'ls_structured_output_format': {'kwargs': {'method': 'json_schema', 'strict': None}, 'schema': {'type': 'function', 'function': {'name': 'answer', 'description': 'see what ar ethe answer that llm gave ', 'parameters': {'type': 'object', 'properties': {'correctness': {'type': 'boolean', 'description': 'Is the submission correct, accurate, and factual?'}}, 'required': ['correctness']}}}}}, config={}, config_factories=[])\n",
       "| JsonOutputParser()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out your prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mensaje': '¡Argh! Sí, soy capitán de este navío y gobierno la cubierta con mano firme. ¿Qué deseas, grumete?',\n",
       " 'correctness': False}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"question\": \"Are you a captain yet?\", \"language\": \"Spanish\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull down a specific commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull down a specific commit from the Prompt Hub by pasting in the code snippet from the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
    "from langsmith import Client\n",
    "\n",
    "prompt = client.pull_prompt(\"pirate-friend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this commit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-CO0ipi385l0nkjGH1km3wsveNyFXU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Arrr, matey! The world of the 2500s be a vast and wondrous place! The skies be filled with airships gliding through the clouds, and the seas be plundered by both pirates and mighty fleets of technology. Cities tower high, made of gleamin' metal and glass, while others be built in harmony with nature, floatin' on water or buried deep underground.\\n\\nTechnology be a double-edged sword, givin' folks the power to explore the stars, yet also creatin' new dangers. Space travel be as common as sailin' the open seas once was, with bounty hunters and treasure seekers searchin' for riches on distant planets.\\n\\nClimate change be a heavy anchor too, matey, with the oceans risin' and new lands takin' shape. Nature and machines clash, but there still be places where the old ways of life, like tradin' and adventurin', thrive.\\n\\nAye, life in the 2500s be filled with adventure and challenges, where the spirit of piracy never truly dies! What be ye interested in, savvy?\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1759838683, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_560af6e559', usage=CompletionUsage(completion_tokens=226, prompt_tokens=33, total_tokens=259, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "hydrated_prompt = prompt.invoke({\"question\": \"What is the world like?\", \"language\": \"English\"})\n",
    "# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\n",
    "converted_messages = convert_prompt_to_openai_format(hydrated_prompt)[\"messages\"]\n",
    "\n",
    "openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=converted_messages,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also easily update your prompts in the hub programmatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/math-prompt/7f6f15fe?organizationId=42e50e11-148e-4f9c-8b19-47e3bd6993cb'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "\n",
    "client=Client()\n",
    "\n",
    "math_prompt = \"\"\"You are an assistant for solving maths equation for students. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "\n",
    "you are a helpful math tutor. Use the following pieces of context to answer the question at the end.\n",
    "\n",
    "Context: {context} \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "math_prompt_template = ChatPromptTemplate.from_template(math_prompt)\n",
    "client.push_prompt(\"math-prompt\", object=math_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also push a prompt as a RunnableSequence of a prompt and a model. This is useful for storing the model configuration you want to use with this prompt. The provider must be supported by the LangSmith playground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/fitness-runnable-sequence/334472c3?organizationId=42e50e11-148e-4f9c-8b19-47e3bd6993cb'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "client=Client()\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "fitness_prompt = \"\"\"You are an assistant for question-answering tasks related to fitness. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "\n",
    "you are a helpful fitness coach. Use the following pieces of context to answer the question at the end.\n",
    "\n",
    "Context: {context} \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "fitness_prompt_template = ChatPromptTemplate.from_template(fitness_prompt)\n",
    "chain = fitness_prompt_template | model\n",
    "client.push_prompt(\"fitness-runnable-sequence\", object=chain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls-academy (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
